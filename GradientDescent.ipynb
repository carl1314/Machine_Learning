{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LjBLincoln/Machine_Learning/blob/master/GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2NarO9hzVmW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **GradientDescent**"
      ]
    },
    {
      "metadata": {
        "id": "Hvbr9oAsWem8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **1 Dimension**"
      ]
    },
    {
      "metadata": {
        "id": "3-Z0mDfucqB6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "f(x) =  x^3 + 2 * x - 3\n",
        "\n",
        "f(x)'s derivative(x) = 6 * x^5 + 16 * x^3 - 18 * x^2 + 8 * x - 12, \n",
        "\n",
        "#x= 1, derivative(x) = 0  \n",
        "\n",
        "当x < 1时，derivative < 0，斜率为负的；\n",
        "当x > 1时，derivative > 0，斜率为正的；\n",
        "当x 无限接近 1时，derivative也就无限=0，斜率为零。\n",
        "```\n",
        "\n",
        "\n",
        "   \n",
        "```\n",
        "\n",
        "x在函数中的移动:\n",
        "\n",
        "x = x - reate * derivative\n",
        "\n",
        "当斜率为负的时候，x增大，当斜率为正的时候，x减小；\n",
        "因此x总是会向着低谷移动，使得error最小，从而求得 f(x) = 0处的解。\n",
        "其中的rate代表x逆着导数方向移动的距离，\n",
        "rate越大，x每次就移动的越多。\n",
        "反之移动的越少。\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "0tdfzCUfWqeS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f4e5decf-b58f-4073-85d7-fe82c13d173a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# f(x) = 0 ,x = ?\n",
        "\n",
        "\n",
        "def f(x):\n",
        "  return x**3 + 2 * x - 3\n",
        "\n",
        "def error(x):\n",
        "  return (f(x) - 0)**2\n",
        "\n",
        "def gradient_descent(x):\n",
        "  delta = 0.00000001\n",
        "  derivative = (error(x + delta) - error(x)) / delta\n",
        "  rate = 0.01\n",
        "  return x - rate * derivative\n",
        "\n",
        "x = 0.8\n",
        "for i in range(50):\n",
        "  x = gradient_descent(x)\n",
        "  #print(x)\n",
        "  if i % 10 == 0 :\n",
        "      print('x = {:6f}, f(x) = {:6f}'.format(x, f(x)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = 0.869619, f(x) = -0.603123\n",
            "x = 0.999795, f(x) = -0.001025\n",
            "x = 1.000000, f(x) = -0.000001\n",
            "x = 1.000000, f(x) = -0.000000\n",
            "x = 1.000000, f(x) = -0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jOCdSnFlWqtF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **n Dimension**"
      ]
    },
    {
      "metadata": {
        "id": "K3bxSPtXd5BB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "f(x) = x[0] + 2 * x[1] + 4\n",
        "\n",
        "要求f(x) = 0处，x[0]与x[1]的值，也可以通过求error函数的最小值来间接求f(x)的解。跟一维函数唯一不同的是，要分别对x[0]与x[1]进行求导。\n",
        "\n",
        "偏导数：\n",
        "\n",
        "保持x[1]不变，对x[0]进行求导，即f(x)对x[0]的偏导数\n",
        "保持x[0]不变，对x[1]进行求导，即f(x)对x[1]的偏导数\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "Scw3hT3lWwGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "1a09ea74-832c-41cf-a80a-6648dfa35018"
      },
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return x[0] + 2 * x[1] + 4\n",
        "\n",
        "def error(x):\n",
        "  return (f(x) - 0)**2\n",
        "\n",
        "def gradient_descent(x):\n",
        "  delta = 0.00000001\n",
        "  derivative_x0 = (error([x[0] + delta, x[1]]) - error([x[0], x[1]])) / delta\n",
        "  derivative_x1 = (error([x[0], x[1] + delta]) - error([x[0], x[1]])) / delta\n",
        "  rate = 0.02\n",
        "  x[0] = x[0] - rate * derivative_x0\n",
        "  x[1] = x[1] - rate * derivative_x1\n",
        "  return [x[0], x[1]]\n",
        "  \n",
        "x = [-0.5, -1.0]\n",
        "for i in range(100):\n",
        "  x = gradient_descent(x)\n",
        "  #print(x)\n",
        "  if i % 10 == 0 :\n",
        "    print('x = {:6f},{:6f}, f(x) = {:6f}'.format(x[0],x[1],f(x)))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = -0.560000,-1.120000, f(x) = 1.200000\n",
            "x = -0.774230,-1.548460, f(x) = 0.128849\n",
            "x = -0.797233,-1.594466, f(x) = 0.013835\n",
            "x = -0.799703,-1.599406, f(x) = 0.001486\n",
            "x = -0.799968,-1.599936, f(x) = 0.000159\n",
            "x = -0.799997,-1.599993, f(x) = 0.000017\n",
            "x = -0.800000,-1.599999, f(x) = 0.000002\n",
            "x = -0.800000,-1.600000, f(x) = 0.000000\n",
            "x = -0.800000,-1.600000, f(x) = 0.000000\n",
            "x = -0.800000,-1.600000, f(x) = -0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72q_z61AWxMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **TensorFlow GradientDescent**\n",
        "\n",
        "```\n",
        "tf.train.GradientDescentOptimizer:\n",
        "\n",
        "Optimizer that implements the gradient descent algorithm.\n",
        "\n",
        "\n",
        "__init__\n",
        "__init__(\n",
        "    learning_rate,\n",
        "    use_locking=False,\n",
        "    name='GradientDescent'\n",
        ")\n",
        "\n",
        "\n",
        "Class Optimizer\n",
        "\n",
        "minimize\n",
        "\n",
        "minimize(\n",
        "    loss,\n",
        "    global_step=None,\n",
        "    var_list=None,\n",
        "    gate_gradients=GATE_OP,\n",
        "    aggregation_method=None,\n",
        "    colocate_gradients_with_ops=False,\n",
        "    name=None,\n",
        "    grad_loss=None\n",
        ")\n",
        "Add operations to minimize loss by updating var_list.\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "wOo9J3bXVm_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "2c960269-21fe-4883-fc06-34c9064ab080"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        " \n",
        "# Model parameters\n",
        "W = tf.Variable([.3], dtype=tf.float32)\n",
        "b = tf.Variable([-.3], dtype=tf.float32)\n",
        "\n",
        "# Model input and output\n",
        "x = tf.placeholder(tf.float32)\n",
        "linear_model = W*x + b\n",
        "y = tf.placeholder(tf.float32)\n",
        " \n",
        "# loss\n",
        "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
        "train = optimizer.minimize(loss)\n",
        " \n",
        "# training data\n",
        "x_train = [1, 2, 3, 4]\n",
        "y_train = [0, -1, -2, -3]\n",
        "\n",
        "# training loop\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "\n",
        "sess.run(init) # reset values to wrong\n",
        "\n",
        "for i in range(1000):\n",
        "  sess.run(train, {x: x_train, y: y_train})\n",
        " \n",
        "  # evaluate training accuracy\n",
        "  curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
        "  if i % 50 == 0 :\n",
        "      print(\"step: %s W: %s b: %s loss: %s\"%(i, curr_W, curr_b, curr_loss))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 W: [-0.21999997] b: [-0.456] loss: 4.0181446\n",
            "step: 50 W: [-0.7127021] b: [0.15530905] loss: 0.47665495\n",
            "step: 100 W: [-0.84270465] b: [0.5375326] loss: 0.14287975\n",
            "step: 150 W: [-0.913881] b: [0.74679965] loss: 0.04282894\n",
            "step: 200 W: [-0.9528499] b: [0.86137295] loss: 0.012838208\n",
            "step: 250 W: [-0.97418535] b: [0.9241017] loss: 0.0038483376\n",
            "step: 300 W: [-0.98586655] b: [0.9584459] loss: 0.0011535526\n",
            "step: 350 W: [-0.992262] b: [0.9772494] loss: 0.00034577597\n",
            "step: 400 W: [-0.9957634] b: [0.98754394] loss: 0.000103651124\n",
            "step: 450 W: [-0.9976805] b: [0.99318033] loss: 3.106893e-05\n",
            "step: 500 W: [-0.99873006] b: [0.9962662] loss: 9.313486e-06\n",
            "step: 550 W: [-0.99930465] b: [0.9979557] loss: 2.7921515e-06\n",
            "step: 600 W: [-0.99961936] b: [0.99888086] loss: 8.3671165e-07\n",
            "step: 650 W: [-0.99979156] b: [0.9993872] loss: 2.5092038e-07\n",
            "step: 700 W: [-0.9998859] b: [0.9996646] loss: 7.514916e-08\n",
            "step: 750 W: [-0.99993753] b: [0.9998163] loss: 2.2558808e-08\n",
            "step: 800 W: [-0.9999658] b: [0.99989945] loss: 6.753911e-09\n",
            "step: 850 W: [-0.9999812] b: [0.9999448] loss: 2.0398154e-09\n",
            "step: 900 W: [-0.9999897] b: [0.9999697] loss: 6.12733e-10\n",
            "step: 950 W: [-0.9999944] b: [0.99998343] loss: 1.8181368e-10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}