{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LjBLincoln/Machine_Learning/blob/master/mnist_softmax.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "hBRFgNnWjq_b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# mnist_softmax"
      ]
    },
    {
      "metadata": {
        "id": "Hgb9YZHdmV_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "\n",
        "def download(directory, filename):\n",
        "  filepath = os.path.join(directory, filename)\n",
        "  if tf.gfile.Exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.gfile.Exists(directory):\n",
        "    tf.gfile.MakeDirs(directory)\n",
        "  url = 'https://storage.googleapis.com/cvdf-datasets/mnist/' + filename + '.gz'\n",
        "  zipped_filepath = filepath + '.gz'\n",
        "  print('Downloading %s to %s' % (url, zipped_filepath))\n",
        "  urllib.request.urlretrieve(url, zipped_filepath)\n",
        "  with gzip.open(zipped_filepath, 'rb') as f_in, open(filepath, 'wb') as f_out:\n",
        "    shutil.copyfileobj(f_in, f_out)\n",
        "  os.remove(zipped_filepath)\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def dataset(directory, images_file, labels_file):\n",
        "  images_file = download(directory, images_file)\n",
        "  labels_file = download(directory, labels_file)\n",
        "\n",
        "  def decode_image(image):\n",
        "    # Normalize from [0, 255] to [0.0, 1.0]\n",
        "    image = tf.decode_raw(image, tf.uint8)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.reshape(image, [784])\n",
        "    return image / 255.0\n",
        "\n",
        "  def decode_label(label):\n",
        "    label = tf.decode_raw(label, tf.uint8)\n",
        "    label = tf.reshape(label, [])\n",
        "    return tf.to_int32(label)\n",
        "\n",
        "  images = tf.data.FixedLengthRecordDataset(\n",
        "      images_file, 28 * 28, header_bytes=16).map(decode_image)\n",
        "  labels = tf.data.FixedLengthRecordDataset(\n",
        "      labels_file, 1, header_bytes=8).map(decode_label)\n",
        "  return tf.data.Dataset.zip((images, labels))\n",
        "\n",
        "\n",
        "def mnist_train(directory):\n",
        "  return dataset(directory, 'train-images-idx3-ubyte',\n",
        "                 'train-labels-idx1-ubyte')\n",
        "\n",
        "def mnist_test(directory):\n",
        "  return dataset(directory, 't10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte')\n",
        "\n",
        "def setup_mnist_data(is_training, hp, batch_size):\n",
        "  if is_training:\n",
        "    ds = mnist_train('/tmp/autograph_mnist_data')\n",
        "    ds = ds.shuffle(batch_size * 10)\n",
        "  else:\n",
        "    ds = mnist_test('/tmp/autograph_mnist_data')\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n",
        "\n",
        "def get_next_batch(ds):\n",
        "  itr = ds.make_one_shot_iterator()\n",
        "  image, label = itr.get_next()\n",
        "  x = tf.to_float(tf.reshape(image, (-1, 28 * 28)))\n",
        "  y = tf.one_hot(tf.squeeze(label), 10)\n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXSB1-Trnj1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b2eac0db-86a8-407f-9a26-de301704f536"
      },
      "cell_type": "code",
      "source": [
        "  # Import data\n",
        "  mnist = input_data.read_data_sets(FLAGS.data_dir)\n",
        "\n",
        "  # Create the model\n",
        "  x = tf.placeholder(tf.float32, [None, 784])\n",
        "  W = tf.Variable(tf.zeros([784, 10]))\n",
        "  b = tf.Variable(tf.zeros([10]))\n",
        "  y = tf.matmul(x, W) + b\n",
        "\n",
        "  # Define loss and optimizer\n",
        "  y_ = tf.placeholder(tf.int64, [None])\n",
        "\n",
        "  # The raw formulation of cross-entropy,\n",
        "  #\n",
        "  #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
        "  #                                 reduction_indices=[1]))\n",
        "  #\n",
        "  # can be numerically unstable.\n",
        "  #\n",
        "  # So here we use tf.losses.sparse_softmax_cross_entropy on the raw\n",
        "  # outputs of 'y', and then average across the batch.\n",
        "  cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\n",
        "  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
        "\n",
        "  \n",
        "  sess = tf.InteractiveSession()\n",
        "  \n",
        "    \n",
        "  train_ds = setup_mnist_data(True, hp, 50)\n",
        "  test_ds = setup_mnist_data(False, hp, 1000)\n",
        "  \n",
        "  \n",
        "  tf.global_variables_initializer().run()\n",
        "  \n",
        "  # Train\n",
        "  for _ in range(1000):\n",
        "    batch_xs, batch_ys = get_next_batch(train_ds)\n",
        "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
        "\n",
        "  # Test trained model\n",
        "  correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  print(sess.run(\n",
        "      accuracy, feed_dict={\n",
        "          x: mnist.test.images,\n",
        "          y_: mnist.test.labels\n",
        "      }))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf7748754ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_mnist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_mnist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hp' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qVDj58IjrNnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}